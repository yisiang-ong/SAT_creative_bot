{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yisiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from nltk.corpus import stopwords\n",
    "import pytorch_lightning as pl\n",
    "import textdistance as td\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "df = pd.read_csv('pre_compute_label.csv', encoding='UTF-8')\n",
    "df = df.drop([\"Unnamed: 1\",\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Response\"].fillna(\"a\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empathy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)\n",
    "\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_, *args):\n",
    "        X, attention_mask = input_\n",
    "        hidden_states = self.base_model(X, attention_mask=attention_mask)\n",
    "\n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:998: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load roberta-model\n",
    "with torch.no_grad():\n",
    "    emp_model = ClassificationModel(AutoModelWithLMHead.from_pretrained(\"roberta-base\").base_model, 2)\n",
    "    emp_model.load_state_dict(torch.load('NLP models/Empathy Classification/empathy_model/RoBERTa_empathy_2ft_2.pt', map_location=torch.device('cpu'))) #change path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import S\n",
    "\n",
    "\n",
    "def empathy_score(text):  # RoBERTa\n",
    "    '''\n",
    "    Computes a discrete numerical empathy score for an utterance (scale 0 to 1)\n",
    "    '''\n",
    "\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # text = text.lower()\n",
    "    t = ByteLevelBPETokenizer(\n",
    "                \"NLP models/Empathy Classification/tokenizer/vocab.json\", #change path\n",
    "                \"NLP models/Empathy Classification/tokenizer/merges.txt\"  #change path\n",
    "            )\n",
    "    t._tokenizer.post_processor = BertProcessing(\n",
    "                (\"</s>\", t.token_to_id(\"</s>\")),\n",
    "                (\"<s>\", t.token_to_id(\"<s>\")),\n",
    "            )\n",
    "    t.enable_truncation(512)\n",
    "    t.enable_padding(pad_id=t.token_to_id(\"<pad>\"))\n",
    "    tokenizer = t\n",
    "    encoded = tokenizer.encode(text)\n",
    "    sequence_padded = torch.tensor(encoded.ids).unsqueeze(0)\n",
    "    attention_mask_padded = torch.tensor(encoded.attention_mask).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = emp_model((sequence_padded, attention_mask_padded))\n",
    "    top_p, top_class = output.topk(1, dim=1)\n",
    "    label = int(top_class[0][0])\n",
    "    # normalise between 0 and 1 dividing by the highest possible value:\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538\n",
      "1538\n"
     ]
    }
   ],
   "source": [
    "emp_list = []\n",
    "for i in df1:\n",
    "    empathy = empathy_score(i[0])\n",
    "    emp_list.append(empathy)\n",
    "print(len(emp_list))\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>empathy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   empathy_score\n",
       "0              0\n",
       "1              1\n",
       "2              1\n",
       "3              1\n",
       "4              1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_score = pd.DataFrame(emp_list,columns=[\"empathy_score\"])\n",
    "emp_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "empathy_score\n",
       "0    645\n",
       "1    893\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_score.groupby(\"empathy_score\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_score.to_csv(\"empathy_score.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluency score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT2 language model weights\n",
    "with torch.no_grad():\n",
    "    gptmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    gptmodel.eval()\n",
    "\n",
    "# Load pre-trained GPT2 tokenizer\n",
    "gpttokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# simple tokenizer + stemmer\n",
    "regextokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "stemmer = nltk.stem.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(sentence):\n",
    "    '''\n",
    "    Computes the PPL of an utterance using GPT2 LM\n",
    "    '''\n",
    "    tokenize_input = gpttokenizer.encode(sentence)\n",
    "    tensor_input = torch.tensor([tokenize_input])\n",
    "    with torch.no_grad():\n",
    "        loss = gptmodel(tensor_input, labels=tensor_input)[0]\n",
    "    return np.exp(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition_penalty(sentence):\n",
    "    '''\n",
    "    Adds a penalty for each repeated (stemmed) token in\n",
    "    an utterance. Returns the total penalty of the sentence\n",
    "    '''\n",
    "    word_list = regextokenizer.tokenize(sentence.lower())\n",
    "    filtered_words = [\n",
    "        word for word in word_list if word not in stopwords.words('english')]\n",
    "    stem_list = [stemmer.stem(word) for word in filtered_words]\n",
    "    penalty = 0\n",
    "    visited = []\n",
    "    for w in stem_list:\n",
    "        if w not in visited:\n",
    "            visited.append(w)\n",
    "        else:\n",
    "            penalty += 0.001\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluency_score(sentence):\n",
    "    '''\n",
    "    Computes the fluency score of an utterance, given by the\n",
    "    inverse of the perplexity minus a penalty for repeated tokens\n",
    "    '''\n",
    "    ppl = perplexity(sentence)\n",
    "    penalty = repetition_penalty(sentence)\n",
    "    score = (1 / ppl) - penalty\n",
    "    # normalise by the highest possible fluency computed on the corpus:\n",
    "    normalised_score = score / 0.09\n",
    "    if normalised_score < 0:\n",
    "        normalised_score = 0\n",
    "    return round(normalised_score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538\n",
      "1538\n"
     ]
    }
   ],
   "source": [
    "fl_list = []\n",
    "for i in df1:\n",
    "    fl_score = fluency_score(i[0])\n",
    "    fl_list.append(fl_score)\n",
    "print(len(fl_list))\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fluency_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fluency_score\n",
       "0           0.03\n",
       "1           0.08\n",
       "2           0.18\n",
       "3           0.16\n",
       "4           0.06"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_scores = pd.DataFrame(fl_list,columns=[\"fluency_score\"])\n",
    "fl_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_scores.to_csv(\"fluency_score.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "     ------------------------------------ 126.0/126.0 KB 185.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vaderSentiment) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vaderSentiment) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.2; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(sentence):\n",
    "    \"\"\"\n",
    "    Compute the sentiment score of the sentence. Scale (-1, 1).\n",
    "    and also normalized to scale of (0,1) by adding 1 and divide by 2. max(0.9836), min(-0.613)\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sent_score = analyzer.polarity_scores(sentence)\n",
    "    sent_score =  (sent_score[\"compound\"] + 0.613)/ (0.9836+0.613)\n",
    "    return sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538\n",
      "1538\n"
     ]
    }
   ],
   "source": [
    "sent_list = []\n",
    "for i in df1:\n",
    "    sent_score = sentiment_score(i[0])\n",
    "    sent_list.append(sent_score)\n",
    "print(len(sent_list))\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.464111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.803896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.941876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.896029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.659777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_score\n",
       "0         0.464111\n",
       "1         0.803896\n",
       "2         0.941876\n",
       "3         0.896029\n",
       "4         0.659777"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_scores = pd.DataFrame(sent_list,columns=[\"sentiment_score\"])\n",
    "sent_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scores.to_csv(\"sentiment_score_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54bb103c3e8827112ac287ff09b16e5ca2d85540a3af0b288083619c88e41aa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
